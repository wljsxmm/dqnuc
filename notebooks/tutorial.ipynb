{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10c35db5",
   "metadata": {},
   "source": [
    "# Getting Started with RL4UC\n",
    "\n",
    "This notebook will briefly describe how to use RL4UC. It will cover the following elements:\n",
    "\n",
    "- Making an environment\n",
    "- Interacting with the environment\n",
    "- Training and testing an agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18459484",
   "metadata": {},
   "source": [
    "## Making an Environment\n",
    "\n",
    "Environments are specified with just a handful of arguments. The key ones are: \n",
    "\n",
    "- `num_gen`: number of controllable generators (must be at least 5)\n",
    "- `dispatch_freq_mins`: length of settlement periods/frequency of decision-making (every 30 minutes by default)\n",
    "- `voll`: the 'value of lost load', determining the magnitude of the penalty for failing to meet demand\n",
    "- `arma_demand` and `arma_wind`: a dictionary specifying the details of the auto-regressive moving average processes determining the forecast errors for demand and wind\n",
    "- `usd_per_kgco2`: the cost in USD of emitting a kilogram of CO2\n",
    "\n",
    "You can manually create an environment with your specifications using `make_env()`, or you can create an environment from a json file using `make_env_from_json()`. The json which must be located in `data/envs`. There are already a few datasets in there for systems ranging from 5 to 30 generators. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0f9908",
   "metadata": {},
   "source": [
    "First we will create an environment manually. All the variables have default values, so we will just specify a few here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ead6ab9",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "make_env() missing 2 required positional arguments: 'mode' and 'profiles_df'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [2]\u001B[0m, in \u001B[0;36m<cell line: 4>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mrl4uc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01menvironment\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m make_env\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m env \u001B[38;5;241m=\u001B[39m \u001B[43mmake_env\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mTypeError\u001B[0m: make_env() missing 2 required positional arguments: 'mode' and 'profiles_df'"
     ]
    }
   ],
   "source": [
    "from rl4uc.environment import make_env\n",
    "import numpy as np\n",
    "\n",
    "env = make_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf31fc4a",
   "metadata": {},
   "source": [
    "The generator specifications are all derived from a single dataset, widely used in the UC literature, from: Kazarlis, S.A., Bakirtzis, A.G. and Petridis, V., 1996. A genetic algorithm solution to the unit commitment problem. IEEE transactions on power systems, 11(1), pp.83-92.\n",
    "\n",
    "For 10 generators, the paper specifies quadratic fuel cost curves, cold and hot start costs (RL4UC currently considers hot starts), minimum up/down times and minimum and maximum operating outputs. \n",
    "\n",
    "Here is the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4feab17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.gen_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c370c8b",
   "metadata": {},
   "source": [
    "When we set `num_gen < 10`, RL4UC takes a subset of these generators by default. When using more than 10 generators, we use duplicates of these generators.\n",
    "\n",
    "如果使用make env 的方法 就是用默认的机组参数进行复制和采样 如果用是make from json 那就是自己自定义机组参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ec49c2",
   "metadata": {},
   "source": [
    "When making an environment from a json, we need to specify the environment's 'name', that is the prefix to '.json'. As mentioned, this must be in the directory `data/envs`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df5e31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl4uc.environment import make_env_from_json\n",
    "\n",
    "env = make_env_from_json('10gen_carbon1')\n",
    "env.gen_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ca863d",
   "metadata": {},
   "source": [
    "The preset environments are aimed at better defining benchmarks that people can compare performance on.\\\n",
    "预设环境旨在更好地定义人们可以比较性能的基准。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01b5241",
   "metadata": {},
   "source": [
    "### Training and Testing Modes\n",
    "\n",
    "Environments have two modes: train and test. Training mode is designed for when we want to sample from a **range of episodes**, specified in `env.profiles_df`. In training mode, when we 'reset' with `env.reset()` the environment samples a new episode from `env.profiles_df`. This makes it useful for training agents. In test mode, we are considering the case where `env.profiles_df` is **a single test episode**, and we want to evaluate the performance. Typically the test problem will not have been observed in training.\\\n",
    "想要写test 只需要把预定义的文件改为 想要测试的某一天或者 某一个时间段 同时  test的数据 在训练集里不应该被agent跑到过  \n",
    "\n",
    "When making an environment in training mode (e.g. `make_env(mode='train')`), the profiles in `data/train_data_10gen.csv` are automatically scaled to the number of generators and available as episodes. These profile are based on data from the GB power system. By contrast, when using a test environment you **must** specify a test profile.\n",
    "\n",
    "Some other differences between training and testing:\n",
    "\n",
    "- In train mode, states are terminal if there is lost load. In testing the episode continues. \n",
    "- In train mode, resetting the environment randomly initialises the generator up/down times. In test mode they are initialised to `env.gen_info.status`\n",
    "env.gen_info.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091d4b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.gen_info.status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9850287b",
   "metadata": {},
   "source": [
    "## Interacting with the environment\n",
    "\n",
    "RL4UC **roughly** follows the OpenAI Gym API. In other words, we act on the environment by executing `env.step(action)`, which returns an observation, reward and an indicator of whether the state is terminal. (Currently the 'info' return is not implemented). We can reset the environment to begin a new episode with `env.reset()`. **An action is a binary sequence of length `env.num_gen`, where 1 indicates turning the generator on, and 2 indicates turning it off.**\n",
    "\n",
    "For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3deee0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env_from_json('5gen') # make the environment\n",
    "obs = env.reset() # reset the environment\n",
    "action = np.array([1,1,1,1,0]) # turn all but the last generator on\n",
    "obs, reward, done = env.step(action) # act on the environment\n",
    "#print(obs)\n",
    "print(\"Reward: {}\".format(reward))\n",
    "print(\"Done: {}\".format(done))\n",
    "print(\"obs:{}\".format(obs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcaef8f",
   "metadata": {},
   "source": [
    "We can see the reward achieved by the action. In addition, we can see whether the environment is 'done': that is, whether it has reached a terminal state. \n",
    "\n",
    "**NOTE: currently (and counter-intuitively) you can still continue to act on an environment after it has reached a terminal state. When training, you should embed each episode in a while loop, that checks for if environment is done (as in the next section).**\n",
    "**注意：目前（并且违反直觉）您仍然可以在环境达到最终状态后继续对其进行操作。训练时，您应该将每一集嵌入到一个 while 循环中，以检查环境是否已完成（如下一节所述）。**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11e721a",
   "metadata": {},
   "source": [
    "### States\n",
    "\n",
    "States are comprised predominantly of the following components: generator up/down times, demand forecast, wind forecast, timestep. In addition are the forecast errors, produced by the ARMA processes. **Note: while the forecast errors are returned in the observation, you may want to consider whether the agent *should* observe this in a particular problem setting. If you are training an agent to solve the day-ahead problem, it probably *shouldn't* observe them, as that won't be possible when it comes to testing!**\n",
    "\n",
    "Let's see the observed state from the code block above:\n",
    "状态主要由以下部分组成：发电机启动/停机时间、需求预测、风力预测、时间步长。此外还有由 ARMA 过程产生的预测误差。注意：虽然在观察中返回了预测误差，但您可能需要考虑代理是否应该在特定问题设置中观察这一点。如果你正在训练一个代理来解决前一天的问题，它可能不应该观察它们，因为这在测试中是不可能的！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61a2718",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af629e4",
   "metadata": {},
   "source": [
    "The `status` component gives the up/down times in decision periods. Remember that the generators have minimum up/down time constraints, so the up/down times are a very important component of the state vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5d35a8",
   "metadata": {},
   "source": [
    "## Training an Agent\n",
    "\n",
    "Now we'll look at how to train an agent. \n",
    "\n",
    "In this case we'll look at a simple Q-learning agent. Needless to say, this is a pretty simple solution that could be vastly improved.\n",
    "\n",
    "**Note: this requires Pytorch to run yourself.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdaaa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class QAgent(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super(QAgent, self).__init__()\n",
    "        self.num_gen = env.num_gen\n",
    "        \n",
    "        self.num_nodes = 32\n",
    "        self.gamma = 0.99\n",
    "        self.activation = torch.tanh\n",
    "        \n",
    "        # There are 2N output nodes, corresponding to ON/OFF for each generator\n",
    "        self.n_out = 2*self.num_gen\n",
    "        \n",
    "        self.obs_size = self.process_observation(env.reset()).size\n",
    "        \n",
    "        self.in_layer = nn.Linear(self.obs_size, self.num_nodes)\n",
    "        self.out_layer = nn.Linear(self.num_nodes, self.n_out) \n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=3e-04)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "                \n",
    "    def process_observation(self, obs):\n",
    "        \"\"\"\n",
    "        Process an observation into a numpy array.\n",
    "        \n",
    "        Observations are given as dictionaries, which is not very convenient\n",
    "        for function approximation. Here we take just the generator up/down times\n",
    "        and the timestep.\n",
    "        \n",
    "        Customise this!\n",
    "        \"\"\"\n",
    "        obs_new = np.concatenate((obs['status'], [obs['timestep']]))\n",
    "        return obs_new\n",
    "    \n",
    "    def forward(self, obs):\n",
    "        x = torch.as_tensor(obs).float()\n",
    "        x = self.activation(self.in_layer(x))\n",
    "        return self.out_layer(x)\n",
    "        \n",
    "    def act(self, obs):\n",
    "        \"\"\"\n",
    "        Agent always acts greedily w.r.t Q-values!\n",
    "        \"\"\"\n",
    "        processed_obs = self.process_observation(obs)\n",
    "\n",
    "        q_values = self.forward(processed_obs)\n",
    "        q_values = q_values.reshape(self.num_gen, 2)\n",
    "        action = q_values.argmax(axis=1).detach().numpy()\n",
    "        \n",
    "        return action, processed_obs\n",
    "    \n",
    "    def update(self, memory, batch_size=None):\n",
    "        \n",
    "        if batch_size == None:\n",
    "            batch_size = memory.capacity\n",
    "        \n",
    "        data = memory.sample(batch_size)\n",
    "        \n",
    "        qs = self.forward(data['obs']).reshape(batch_size, self.num_gen, 2)\n",
    "        \n",
    "        # A bit of complicated indexing here! \n",
    "        # We are using the actions [batch_size, num_gen] to index Q-values\n",
    "        # which have shape [batch_size, num_gen, 2]\n",
    "        m,n = data['act'].shape\n",
    "        I,J = np.ogrid[:m,:n]\n",
    "        qs = qs[I, J, data['act']]\n",
    "        \n",
    "        next_qs = self.forward(data['next_obs']).reshape(batch_size, self.num_gen, 2)\n",
    "        next_acts = next_qs.argmax(axis=2).detach().numpy()\n",
    "        \n",
    "        # The same complicated indexing! \n",
    "        m,n = next_acts.shape\n",
    "        I,J = np.ogrid[:m,:n]\n",
    "        next_qs = next_qs[I, J, next_acts]\n",
    "        \n",
    "        # Recasting rewards into the same shape as next_qs\n",
    "        m,n = next_qs.shape\n",
    "        rews = np.broadcast_to(data['rew'], (self.num_gen,batch_size)).T\n",
    "        rews = torch.as_tensor(rews).float()\n",
    "\n",
    "        td_target = rews + self.gamma * next_qs\n",
    "                \n",
    "        criterion = nn.MSELoss()\n",
    "        loss = criterion(qs, td_target)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity, obs_size, act_dim):\n",
    "        \n",
    "        self.capacity = capacity\n",
    "        self.obs_size = obs_size\n",
    "        self.act_dim = act_dim \n",
    "        \n",
    "        self.act_buf = np.zeros((self.capacity, self.act_dim))\n",
    "        self.obs_buf = np.zeros((self.capacity, self.obs_size))\n",
    "        self.rew_buf = np.zeros(self.capacity)\n",
    "        self.next_obs_buf = np.zeros((self.capacity, self.obs_size))\n",
    "        \n",
    "        self.num_used = 0\n",
    "        \n",
    "    def store(self, obs, action, reward, next_obs):\n",
    "        \"\"\"Store a transition in the memory\"\"\"\n",
    "        idx = self.num_used % self.capacity\n",
    "        \n",
    "        self.act_buf[idx] = action\n",
    "        self.obs_buf[idx] = obs\n",
    "        self.rew_buf[idx] = reward\n",
    "        self.next_obs_buf[idx] = next_obs\n",
    "        \n",
    "        self.num_used += 1\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(self.capacity), size=batch_size, replace=False)\n",
    "        \n",
    "        data = {'act': self.act_buf[idx],\n",
    "                'obs': self.obs_buf[idx],\n",
    "                'rew': self.rew_buf[idx],\n",
    "                'next_obs': self.next_obs_buf[idx]}\n",
    "        \n",
    "        return data\n",
    "        \n",
    "    def is_full(self):\n",
    "        return (self.num_used >= self.capacity)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.num_used = 0 \n",
    "        \n",
    "def train():\n",
    "    \n",
    "    MEMORY_SIZE = 200\n",
    "    N_EPOCHS = 500\n",
    "    \n",
    "    env = make_env_from_json('5gen')\n",
    "    agent = QAgent(env)\n",
    "    memory = ReplayMemory(MEMORY_SIZE, agent.obs_size, env.num_gen)\n",
    "    \n",
    "    log = {'mean_timesteps': [],\n",
    "           'mean_reward': []}\n",
    "    \n",
    "    for i in range(N_EPOCHS):\n",
    "        if i % 10 == 0:\n",
    "            print(\"Epoch {}\".format(i))\n",
    "        epoch_timesteps = []\n",
    "        epoch_rewards = []\n",
    "        while memory.is_full() == False:\n",
    "            done = False\n",
    "            obs = env.reset()\n",
    "            timesteps = 0\n",
    "            while not done: \n",
    "                action, processed_obs = agent.act(obs)\n",
    "                next_obs, reward, done = env.step(action)\n",
    "                \n",
    "                next_obs_processed = agent.process_observation(next_obs)\n",
    "                \n",
    "                memory.store(processed_obs, action, reward, next_obs_processed)\n",
    "                \n",
    "                obs = next_obs\n",
    "                \n",
    "                if memory.is_full():\n",
    "                    break\n",
    "                \n",
    "                timesteps += 1\n",
    "                if done:\n",
    "                    epoch_rewards.append(reward)\n",
    "                    epoch_timesteps.append(timesteps)\n",
    "                    \n",
    "        log['mean_timesteps'].append(np.mean(epoch_timesteps))\n",
    "        log['mean_reward'].append(np.mean(epoch_rewards))\n",
    "        \n",
    "        agent.update(memory)\n",
    "        memory.reset()\n",
    "                    \n",
    "    return agent, log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339c7792",
   "metadata": {},
   "source": [
    "The following cell trains the agent. Note that this might take a few minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d848cfed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent, log = train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913d82a0",
   "metadata": {},
   "source": [
    "And now let's see if the agent managed to improve its performance: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa781981",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "pd.Series(log['mean_reward']).rolling(50).mean().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb780bb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Some things you may want to consider when writing an agent:\n",
    "\n",
    "- The action space is combinatorial\n",
    "- Illegal actions are corrected by the environment\n",
    "- Observations and rewards are unnormalised by default"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.8_tf2.6_torch1.8",
   "language": "python",
   "name": "py3.8_tf2.6_torch1.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}